### 小清单

- [ ] 当前任务：**把评论数据集划分为用户评论集和物品评论集**，并使用embedding技术转化为**用户的深度特征**和**物品的深度特征**。
    - [ ] 将评论数据集划分为用户评论集合物品评论集
    - [ ] 使用某种词嵌入技术把**评论文本转化为特征向量。** BERT、Word2vec、sentenceBERT
- [ ] 亚马逊发布了最新的Amazon Reviews Dataset：https://amazon-reviews-2023.github.io/
- [ ] 极其稀疏的矩阵不适合使用pivot？那我之前好像就是使用pivot来保存的数据。
    - [ ] 之前所用的数据是**5-core数据集的子集**，这次使用**其他方法保存数据**，不用透视表保存，因为透视表浪费空间。
- [ ] 将数据划分为训练集、验证集和测试集，然后保存为CSV文件。这样在读取的时候，或许更方便。
1. 有的时候，心里会莫名蹦出一些想法来。这些就是灵感迸发的时刻，要赶紧写下来，理清思路，也许就能当做自己生命经验的一部分。大多时候，我们脑子里的想法是杂乱无章的，逻辑混乱的。当我们尝试把想法用文字写下来的时候，这时候我们更容易看到想法的不成熟之处，然后在重新整理文字的同时，也是对内心想法的一次再整理。语言的边界就是思想的边界，至少对于文字而言是这样的。
2. 






### 影视综
1. 《万物生灵》
   - 这部剧，讲的是兽医专业的男主来到约克郡乡下当兽医的故事。这部剧的节奏很慢，画面很美，很适合心情不好的时候自己一个人看。跟着这部剧看看英国村庄的的小路，看看绿油油的草场和可爱的动物，看看呆呆的男主和他所爱的小动物之间的治愈故事。每一个故事都不长，但是角色的成长却在悄然进行。
   - 故事的节奏虽然很慢，但并不感到乏味，相反有些情节让我在屏幕前笑的前仰后翻，捧腹大笑。每一个笑点都设置的足够生活化，似乎是每一个农村孩子都能察觉到的到的童年经历。 

### 小tips

1. 输入公式不用愁
 $ L = \min(\lambda_r L_r + \lambda_e L_e + \lambda_\theta ||\theta||^2) $



 ### 写点论文

 将本文模型MODEL在评分预测任务上的结果同其他模型进行比较；将本文模型MODEL在解释文本生成任务上的结果同其他模型进行比较。通过实验来证明将这两个任务结合起来进行联合学习的效果要优于单任务学习。

 1. 对比模型
 - 评分预测模型
 - 1. SVD++：一种协同过滤模型，通过在原始的矩阵分解模型中加入用户和项目的隐式反馈，使分解后的隐因子向量容纳更多的隐藏信息。
 - 2. BiasSVD:
 - 3. FunkSVD:
 - 4. NMF:
 - 5. NRT：一种多任务学习模型，使用MLP将用户和物品编码为特征向量，再经由GRU(门控循环单元)生成抽象的解释文本。
 - 6. DeepCoNN：**双塔模型**
 - 文本生成模型
 - 1. CAML:
 - 2. NRT:
  
